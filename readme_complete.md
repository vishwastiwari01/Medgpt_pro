# ğŸ©º MedGPT - Medical Knowledge Assistant

Professional medical RAG (Retrieval-Augmented Generation) assistant with semantic search, AI-powered responses, and PDF document viewer.

[![Streamlit](https://img.shields.io/badge/Streamlit-FF4B4B?style=for-the-badge&logo=Streamlit&logoColor=white)](https://streamlit.io)
[![Python](https://img.shields.io/badge/Python-3.9+-3776AB?style=for-the-badge&logo=python&logoColor=white)](https://python.org)
[![LangChain](https://img.shields.io/badge/LangChain-121212?style=for-the-badge&logo=chainlink&logoColor=white)](https://langchain.com)

## âœ¨ Features

- ğŸ” **Semantic Search** - Find relevant medical information using AI-powered embeddings
- ğŸ¤– **Multiple LLM Backends** - Groq (recommended), OpenAI, Ollama, or smart fallback
- ğŸ“š **Source Citations** - Every answer linked to source documents with page numbers
- ğŸ“„ **PDF Viewer** - Inline document viewing with highlighted relevant sections
- ğŸ“¤ **File Upload** - Add new medical documents dynamically
- â˜ï¸ **Cloud Ready** - Optimized for Streamlit Cloud deployment
- âš¡ **Fast** - Groq API provides 800+ tokens/second inference speed

## ğŸš€ Quick Start

### Local Setup (5 minutes)

```bash
# 1. Clone the repository
git clone https://github.com/your-username/Med_GPT.git
cd Med_GPT

# 2. Install dependencies
pip install -r requirements.txt

# 3. Set up environment
cp .env.template .env
# Edit .env and add your Groq API key

# 4. Prepare documents
mkdir documents
# Add your medical PDFs to documents/
python utils/document_processor.py

# 5. Run the app
streamlit run app.py
```

### Get Free Groq API Key

1. Visit [console.groq.com](https://console.groq.com)
2. Sign up (it's free!)
3. Create an API key
4. Add to `.env`: `GROQ_API_KEY="gsk_your_key"`

## ğŸ“ Project Structure

```
Med_GPT/
â”œâ”€â”€ app.py                      # Main Streamlit application
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ packages.txt               # System packages (for Streamlit Cloud)
â”œâ”€â”€ .env.template              # Environment variables template
â”œâ”€â”€ .streamlit/
â”‚   â””â”€â”€ config.toml           # Streamlit configuration
â”œâ”€â”€ documents/                 # Your medical PDFs (local)
â”œâ”€â”€ vectorstore/              # FAISS vector database
â”‚   â”œâ”€â”€ index.faiss
â”‚   â””â”€â”€ index.pkl
â”œâ”€â”€ student_uploads/          # User-uploaded documents
â””â”€â”€ utils/
    â”œâ”€â”€ llm_handler.py        # LLM backend management
    â”œâ”€â”€ gdrive_loader.py      # HuggingFace vectorstore download
    â”œâ”€â”€ document_processor.py # PDF processing & indexing
    â””â”€â”€ upload_handler.py     # File upload interface

# Documentation
â”œâ”€â”€ README.md                  # This file
â”œâ”€â”€ SETUP_GUIDE.md            # Detailed setup instructions
â”œâ”€â”€ QUICKSTART.md             # 5-minute quick start
â””â”€â”€ STREAMLIT_CLOUD_DEPLOYMENT.md  # Cloud deployment guide

# Setup Scripts
â”œâ”€â”€ setup.py                  # Automated setup script
â””â”€â”€ check_deployment.py       # Pre-deployment validation
```

## ğŸ› ï¸ Technology Stack

- **Frontend**: Streamlit
- **LLM**: Groq API (Llama 3.3 70B), OpenAI GPT-3.5, Ollama (local)
- **Embeddings**: sentence-transformers/all-MiniLM-L6-v2
- **Vector Store**: FAISS
- **Document Processing**: PyMuPDF, PyPDF2, python-docx
- **Framework**: LangChain

## ğŸ“‹ Requirements

- Python 3.9 or higher
- 4GB RAM minimum (8GB recommended)
- Internet connection (for API calls)
- Git (for version control)

## ğŸ¯ Usage

### 1. Ask a Question

Type a medical query in the search box:
```
"What are the treatment options for acute myocardial infarction?"
"Explain the mechanism of action of ACE inhibitors"
"What are the diagnostic criteria for diabetes?"
```

### 2. View Sources

Click on any source reference to:
- View the exact excerpt used
- See the PDF page with highlighted text
- Navigate through the document

### 3. Upload Documents

Use the sidebar to:
- Upload new PDFs or DOCX files
- Process and add to knowledge base
- Immediately available for search

## ğŸŒ Streamlit Cloud Deployment

### Quick Deploy

1. **Push to GitHub**
   ```bash
   git push origin main
   ```

2. **Deploy on Streamlit Cloud**
   - Go to [share.streamlit.io](https://share.streamlit.io)
   - Click "New app"
   - Select your repository
   - Add API key in Secrets:
     ```toml
     GROQ_API_KEY = "gsk_your_key"
     ```

3. **Done!** Your app is live at `your-app.streamlit.app`

### Pre-Deployment Check

```bash
python check_deployment.py
```

This validates:
- âœ… All required files present
- âœ… Git repository configured
- âœ… Dependencies listed
- âœ… Vectorstore ready
- âœ… Configuration files in place

See [STREAMLIT_CLOUD_DEPLOYMENT.md](STREAMLIT_CLOUD_DEPLOYMENT.md) for detailed instructions.

## ğŸ”§ Configuration

### LLM Backend Priority

1. **Groq API** (Default) - Fast, free tier available
2. **OpenAI API** - Reliable, paid service
3. **Ollama** - Local, private (local deployment only)
4. **Fallback** - Context extraction without LLM

Configure in `utils/llm_handler.py`.

### Adjust Search Parameters

In `app.py`:
```python
TOP_K = 3  # Number of relevant chunks to retrieve
```

In `utils/document_processor.py`:
```python
CHUNK_SIZE = 1000      # Size of text chunks
CHUNK_OVERLAP = 200    # Overlap between chunks
```

## ğŸ“Š Performance

- **Groq API**: 800+ tokens/second
- **Local Embeddings**: ~100ms per query
- **Vector Search**: <50ms for 10K documents
- **PDF Rendering**: Instant with caching

## ğŸ› Troubleshooting

### "Groq API initialization failed"

**Solution**: 
1. Check your API key in `.env` or Streamlit Secrets
2. Verify key starts with `gsk_`
3. Test at [console.groq.com](https://console.groq.com)

### "No vectorstore found"

**Solution**:
```bash
# Option 1: Build locally
mkdir documents
# Add PDFs to documents/
python utils/document_processor.py

# Option 2: Configure HuggingFace download
# Edit utils/gdrive_loader.py with your repo_id
```

### "PDF preview not available"

**Expected on Streamlit Cloud**. The app automatically uses:
1. PyMuPDF rendering (local)
2. Base64 embedded viewer (cloud)
3. Download link (fallback)

### "Limited Mode" - No AI responses

**Solution**: Configure an API key
- Preferred: Groq API (free, fast)
- Alternative: OpenAI API
- The app works without AI but responses are basic

## ğŸ“š Documentation

- ğŸ“– [SETUP_GUIDE.md](SETUP_GUIDE.md) - Comprehensive setup instructions
- âš¡ [QUICKSTART.md](QUICKSTART.md) - 5-minute quick start
- â˜ï¸ [STREAMLIT_CLOUD_DEPLOYMENT.md](STREAMLIT_CLOUD_DEPLOYMENT.md) - Cloud deployment
- ğŸ”§ Run `python setup.py` for automated setup
- âœ… Run `python check_deployment.py` before deploying

## ğŸ¤ Contributing

Contributions are welcome! Please:

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Test locally and on Streamlit Cloud
5. Submit a pull request

## âš ï¸ Important Notes

- **Educational Use Only** - Not for clinical decision-making
- **Verify Information** - Always cross-reference medical information
- **Privacy** - Keep API keys private, never commit to Git
- **Data Security** - Don't upload sensitive patient information

## ğŸ“ License

This project is provided as-is for educational purposes.

## ğŸ™ Acknowledgments

- **Groq** - For providing fast, free LLM inference
- **Streamlit** - For the amazing framework
- **LangChain** - For RAG tooling
- **HuggingFace** - For embeddings models

## ğŸ“§ Support

- ğŸ“– Check documentation first
- ğŸ› Report issues on GitHub
- ğŸ’¬ Ask questions in discussions
- ğŸŒŸ Star the repo if you find it useful!

---

**Built with â¤ï¸ for medical education**

**Remember**: This is an educational tool. Always verify medical information with qualified healthcare professionals.
